# -*- coding: utf-8 -*-
"""U-net_4.5layers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XhH5w1bV4tAs0o82SrDpKxbIIFUKfRUI
"""

import tensorflow as tf 
import keras
import numpy as np
import h5py
import random
import scipy.io as sio
from matplotlib import pyplot as plt
from random import randint
import os
import random
import math

from keras.models import Model
from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, Add
from keras.layers import LeakyReLU
from keras.initializers import Constant
from keras.models import load_model

from sklearn.model_selection import train_test_split,cross_val_score
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import make_scorer
from keras import backend as K
K.set_image_data_format('channels_last')
tf.to_int=lambda x: tf.cast(x, tf.int32)

#  Define loss function
def loss_fn(y_true, y_pred):
    # Adjust for class imbalances by giving more weighting to the tumor class when computing the cost
    y_true = tf.compat.v1.to_int32(tf.reshape(y_true, shape=[-1, 128, 128]))  # labels, shape=[batch_size, height, width]
    class_weights = tf.add(tf.compat.v1.to_float(tf.multiply(y_true, 2)), tf.compat.v1.to_float(tf.equal(y_true, 0)))
    # weighting on foreground class and on background class
    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))

# Define DICE as accuracy metric
def dice_coef(y_true, y_pred):
    print(y_pred.shape)
    y_pred = tf.compat.v1.to_float(K.argmax(y_pred, axis=3))
    y_true = tf.reshape(y_true, shape=[-1, 128, 128])
    intersection = K.sum(y_true * y_pred, axis=[1, 2])
    union = K.sum(y_true, axis=[1, 2]) + K.sum(y_pred, axis=[1, 2])
    return K.mean((2. * intersection + 1) / (union + 1), axis=0)  # average dice over examples

# Repeating layers throughout the network 
def add_common_layers(filters, layer, bias_ct=0.05, leaky_alpha=0.03, drop_prob=0.05):
    layer = Conv2D(filters, (3, 3), # num. of filters and kernel size 
                   strides=1,
                   padding='same',
                   use_bias=True,
                   kernel_initializer='glorot_normal', 
                   bias_initializer=Constant(value=bias_ct))(layer)
    layer = LeakyReLU(alpha=leaky_alpha)(layer) # activation function
    layer = Dropout(drop_prob)(layer) 
    return layer

def get_cnn(num_classes=2):

    # num_classes - Tumor segmentation has two classes - "lesion" and "no lesion" pixels

    # This model has skip connections in place, here we use element-wise addition.

    # Define Convolutional Neural Network

    # Input shape 
    input = Input(shape=(128,128,1))

    # Conv1
    x = add_common_layers(16, input)

    # Conv2
    conv2 = add_common_layers(16, x)
    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(conv2)

    # Conv3
    x = add_common_layers(32, x)

    # Conv4
    conv4 = add_common_layers(32, x)
    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(conv4)

    # Conv5
    x = add_common_layers(64, x)

    # Conv6
    x = add_common_layers(64, x)

    # Conv7
    conv7 = add_common_layers(64, x)
    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(conv7)

    # Conv8
    x = add_common_layers(128, x)

    # Conv9
    x = add_common_layers(128, x)

    # Conv10
    conv10 = add_common_layers(128, x)
    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(conv10)

    # Conv8
    x = add_common_layers(256, x)

    # Conv9
    x = add_common_layers(256, x)    

    # Transposed Convolution (upsampling)
    x = Conv2DTranspose(128, (2,2), strides=(2,2), padding='same', use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(value=0.03))(x)
    x = LeakyReLU(alpha=0.01)(x)

    # Conv11
    x = Add()([x, conv10])
    x = add_common_layers(128, x)

    # Conv12
    x = add_common_layers(128, x)

    # Conv13
    x = add_common_layers(128, x)

    # Transposed Convolution (upsampling)
    x = Conv2DTranspose(64, (2,2), strides=(2,2), padding='same', use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(value=0.03))(x)
    x = LeakyReLU(alpha=0.01)(x)

    # Conv14
    x = Add()([x, conv7])
    x = add_common_layers(64, x)

    # Conv15
    x = add_common_layers(64, x)

    # Conv16
    x = add_common_layers(64, x)

    # Transposed Convolution (upsampling)
    x = Conv2DTranspose(32, (2,2), strides=(2,2), padding='same', use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(value=0.03))(x)
    x = LeakyReLU(alpha=0.01)(x)

    # Conv17
    x = Add()([x, conv4])
    x = add_common_layers(32, x)

    # Conv18
    x = add_common_layers(32, x)

    # Transposed Convolution (upsampling)
    x = Conv2DTranspose(16, (2,2), strides=(2,2), padding='same', use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(value=0.03))(x)
    x = LeakyReLU(alpha=0.01)(x)

    # Conv19
    x = Add()([x, conv2])
    x = add_common_layers(16, x)

    # Conv20
    x = Conv2D(num_classes, (3,3), strides=1, padding='same', use_bias=True, kernel_initializer='glorot_normal', bias_initializer=Constant(value=0.03))(x)
    x = LeakyReLU(alpha=0.01)(x)

    output = x

    model = Model(inputs=[input], outputs=[output])

    model.compile(loss=loss_fn, optimizer='adam', metrics=[dice_coef]) 

    return model

# Load training and testing data 
os.chdir('/content/drive/MyDrive/Colab Notebooks/generalizability_Unet_ACRIN')
os.getcwd()
id = np.array([13, 34, 35, 36, 38, 39, 47, 53, 66, 73, 75, 80, 95, 102, 104, 105, 107, 112, 113, 115, 117, 
              121, 124, 133, 140, 149, 155, 171, 172, 181, 191, 196, 198, 202, 208, 211, 219,222])

train_id,test_id,temp1,temp2 = train_test_split(id,id,test_size=0.2, random_state=42)

# 1st CV
model1 = get_cnn()
train_labels = train_id
valid_labels = test_id

X_tot = sio.loadmat('scanner1_X/pat_'+str(train_labels[0])+'_PET_img.mat')
Y_tot = sio.loadmat('scanner1_Y/pat_'+str(train_labels[0])+'_disc_gt.mat')
X_tot = X_tot['PET_img']
Y_tot = Y_tot['disc_gt']


for n in range(len(train_labels)-1):
  PET_X = sio.loadmat('scanner1_X/pat_'+str(train_labels[n+1])+'_PET_img.mat')
  PET_Y = sio.loadmat('scanner1_Y/pat_'+str(train_labels[n+1])+'_disc_gt.mat')
  X_tot = np.concatenate((X_tot, PET_X['PET_img']), axis=2)
  Y_tot = np.concatenate((Y_tot, PET_Y['disc_gt']), axis=2)

X = np.zeros((len(X_tot[0,0,:]),128,128))
Y = np.zeros((len(X_tot[0,0,:]),128,128,2))
for a in range(len(X_tot[0,0,:])):
  X[a,:,:] = X_tot[:,:,a]
  Y[a,:,:,:] = Y_tot[:,:,a,:]
X = np.expand_dims(X,axis=3)

Xtrain1 = np.zeros((len(X[:,0,0,0]),128,128,1))
Ytrain1 = np.zeros((len(X[:,0,0,0]),128,128))
for d in range(len(X[:,0,0,0])):
  Xtrain1[d,:,:,:] = X[d,:,:,:]
  Ytrain1[d,:,:] = Y[d,:,:,0]

X_tot = sio.loadmat('scanner1_X/pat_'+str(valid_labels[0])+'_PET_img.mat')
Y_tot = sio.loadmat('scanner1_Y/pat_'+str(valid_labels[0])+'_disc_gt.mat')
X_tot = X_tot['PET_img']
Y_tot = Y_tot['disc_gt']

for n in range(len(valid_labels)-1):
  PET_X = sio.loadmat('scanner1_X/pat_'+str(valid_labels[n+1])+'_PET_img.mat')
  PET_Y = sio.loadmat('scanner1_Y/pat_'+str(valid_labels[n+1])+'_disc_gt.mat')
  X_tot = np.concatenate((X_tot, PET_X['PET_img']), axis=2)
  Y_tot = np.concatenate((Y_tot, PET_Y['disc_gt']), axis=2)

X = np.zeros((len(X_tot[0,0,:]),128,128))
Y = np.zeros((len(X_tot[0,0,:]),128,128,2))

for a in range(len(X_tot[0,0,:])):
  X[a,:,:] = X_tot[:,:,a]
  Y[a,:,:,:] = Y_tot[:,:,a,:]
X = np.expand_dims(X,axis=3)

Xtest1 = np.zeros((len(X[:,0,0,0]),128,128,1)) 
Ytest1 = np.zeros((len(X[:,0,0,0]),128,128))
for d in range(len(X[:,0,0,0])):
  Xtest1[d,:,:,:] = X[d,:,:,:]
  Ytest1[d,:,:] = Y[d,:,:,0]

callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)

result1 = model1.fit(Xtrain1, Ytrain1, batch_size=4, epochs=25, verbose=1,validation_data=(Xtest1, Ytest1),callbacks = callback)

loss1 = result1.history.get("loss")
dice_score1 = result1.history.get("dice_coef")
valid_loss1 = result1.history.get("val_loss")
dice_valid1 = result1.history.get('val_dice_coef')

ax1 = plt.subplot(2,2,1)
ax1.plot(range(len(loss1)),loss1, label='Training Loss')
ax2 = plt.subplot(2,2,2)
ax2.plot(range(len(loss1)),dice_score1, label='Dice Score on Training Set')
ax3 = plt.subplot(2,2,3)
ax3.plot(range(len(loss1)),valid_loss1, label='Training Loss')
ax4 = plt.subplot(2,2,4)
ax4.plot(range(len(loss1)),dice_valid1, label='Dice Score on Validation Set')

y_pred = model1.predict(Xtest1)
score = dice_coef(tf.compat.v1.to_float(Ytest1), y_pred)
score

id2 = np.array([214,200,199,193,192,178,177,152,151,126,125,109,87,85,83,74,67,51,48,44,41,40,30,25,22,20,19,15,14,12,7,6])

train_id2,test_id2,temp1,temp2 = train_test_split(id2,id2,test_size=0.7, random_state=42)

train_labels = train_id2
valid_labels = test_id2

X_tot = sio.loadmat('scanner2_X/pat_'+str(train_labels[0])+'_PET_img.mat')
Y_tot = sio.loadmat('scanner2_Y/pat_'+str(train_labels[0])+'_disc_gt.mat')
X_tot = X_tot['PET_img']
Y_tot = Y_tot['disc_gt']


for n in range(len(train_labels)-1):
  PET_X = sio.loadmat('scanner2_X/pat_'+str(train_labels[n+1])+'_PET_img.mat')
  PET_Y = sio.loadmat('scanner2_Y/pat_'+str(train_labels[n+1])+'_disc_gt.mat')
  X_tot = np.concatenate((X_tot, PET_X['PET_img']), axis=2)
  Y_tot = np.concatenate((Y_tot, PET_Y['disc_gt']), axis=2)

X = np.zeros((len(X_tot[0,0,:]),128,128))
Y = np.zeros((len(X_tot[0,0,:]),128,128,2))
for a in range(len(X_tot[0,0,:])):
  X[a,:,:] = X_tot[:,:,a]
  Y[a,:,:,:] = Y_tot[:,:,a,:]
X = np.expand_dims(X,axis=3)

Xtrain2 = np.zeros((len(X[:,0,0,0]),128,128,1))
Ytrain2 = np.zeros((len(X[:,0,0,0]),128,128))
for d in range(len(X[:,0,0,0])):
  Xtrain2[d,:,:,:] = X[d,:,:,:]
  Ytrain2[d,:,:] = Y[d,:,:,0]

X_tot = sio.loadmat('scanner2_X/pat_'+str(valid_labels[0])+'_PET_img.mat')
Y_tot = sio.loadmat('scanner2_Y/pat_'+str(valid_labels[0])+'_disc_gt.mat')
X_tot = X_tot['PET_img']
Y_tot = Y_tot['disc_gt']

for n in range(len(valid_labels)-1):
  PET_X = sio.loadmat('scanner2_X/pat_'+str(valid_labels[n+1])+'_PET_img.mat')
  PET_Y = sio.loadmat('scanner2_Y/pat_'+str(valid_labels[n+1])+'_disc_gt.mat')
  X_tot = np.concatenate((X_tot, PET_X['PET_img']), axis=2)
  Y_tot = np.concatenate((Y_tot, PET_Y['disc_gt']), axis=2)

X = np.zeros((len(X_tot[0,0,:]),128,128))
Y = np.zeros((len(X_tot[0,0,:]),128,128,2))

for a in range(len(X_tot[0,0,:])):
  X[a,:,:] = X_tot[:,:,a]
  Y[a,:,:,:] = Y_tot[:,:,a,:]
X = np.expand_dims(X,axis=3)

Xtest2 = np.zeros((len(X[:,0,0,0]),128,128,1)) 
Ytest2 = np.zeros((len(X[:,0,0,0]),128,128))
for d in range(len(X[:,0,0,0])):
  Xtest2[d,:,:,:] = X[d,:,:,:]
  Ytest2[d,:,:] = Y[d,:,:,0]

result2 = model1.fit(Xtrain2, Ytrain2, batch_size=4, epochs=10, verbose=1)

y_pred2 = model1.predict(Xtest2)
score = dice_coef(tf.compat.v1.to_float(Ytest2), y_pred2)
score

plt.imshow(y_pred2[145,:,:,0])

plt.imshow(Ytest2[145,:,:])